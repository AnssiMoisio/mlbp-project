{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning Basic Principles 2018 - Data Analysis Project Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Using neural networks for music classification* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project implements a deep neural network model using Keras. We wanted to find out how well a neural network performs on high-dimensional data with minimal tuning. The same classifier was used for both accuracy and logarithmic loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Background, problem statement, motivation, many references, description of\n",
    "contents. Introduces the reader to the topic and the broad context within which your\n",
    "research/project fits*\n",
    "\n",
    "*- What do you hope to learn from the project?*\n",
    "*- What question is being addressed?*\n",
    "*- Why is this task important? (motivation)*\n",
    "\n",
    "*Keep it short (half to 1 page).*\n",
    "\n",
    "Building a classifier nowadays is easier than ever, though building a good classifier is still a hard task despite the high availability of efficient tools created for this purpose. We wanted to investigate the performance of a minimally-tuned neural network in classification using out-of-the-box libraries. We chose Keras because of its popularity and documentation availability.\n",
    "\n",
    "The data usually has many problems some of which cannot be known beforehand. Preprocessing and visualisation are necessary for building a proper model. However, data analysis is already quite ubiquitous and the need for better tools is ever-growing. Some people, who would benefit from machine learning and data analysis, might not even be aware of the current progress of the field nor the tools at all. If these people were made aware of the field it will overall benefit the society we live in. To this day, it still requires a data scientist/engineer to build the models because of the various bottlenecks present in data science. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Briefly describe data (class distribution, dimensionality) and how will it affect\n",
    "classification. Visualize the data. Donâ€™t focus too much on the meaning of the features,\n",
    "unless you want to.*\n",
    "\n",
    "*- Include histograms showing class distribution.*\n",
    "\n",
    "The supplied data contains 4363 labeled samples and 6544 unlabeled sample vectors. Every sample is a vector of length 264 which is composed of preprocessed properties of the original time-series data. There are 10 labels and the class distribution is unbalanced; almost half of the labeled samples have the same label which could lead to problems in classification. For example, we don't want the classifier to learn the class distribution of the training set because the class distribution might be completely different for the evaluation set. Some of the fields in the data contain almost identical values for the whole dataset which means these features cannot be used to differentiate samples. Some of the features may also be redundant and contribute nothing while having high variance amongst the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_data.csv', header=None).values\n",
    "labels = pd.read_csv('data/train_labels.csv', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the label distribution of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot label distribution in training data\n",
    "plt.figure(1, figsize=(8, 8))\n",
    "plt.hist(labels, range=(0.5,10.5), bins=10, ec='black')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the class imbalance can be seen clearly. It is also good to find out if the data is ordered to know if it needs to suffled before dividing it to training data and validation data. Here is a simple plot of the distribution of the labels in the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(15, 5))\n",
    "plt.plot(labels, marker='.', linestyle = 'None')\n",
    "plt.xlabel('index in csv file')\n",
    "plt.ylabel('label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Pop-rock songs are first in the file and the rest of the genres are distributed randomly after that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can understand the dimensionality better by principal components analysis. Here is a class that is created on basis of the last Python exercise of this course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, data, d):\n",
    "        self.data = data # raw data\n",
    "        self.d = d # number of dimensions in the compressed data\n",
    "        self.W_pca, self.eigvalues = self.compute_pca()\n",
    "\n",
    "    def compute_pca(self):\n",
    "        # Output: a d by D matrix W_pca, and all eigenvalues of Q\n",
    "\n",
    "        N = self.data.shape[0]\n",
    "        # step1: compute the sample cov. matrix Q\n",
    "        Q = np.matmul(np.transpose(self.data), self.data ) / N\n",
    "        #step2: compute the eigenvalues and eigenvectors\n",
    "        w, v = np.linalg.eig(Q)\n",
    "        #step3: Sort the eigenvectors by decreasing eigenvalues, choose the d largest eigenvalues, form W_pca\n",
    "        ind = np.argsort(w)[::-1]\n",
    "        W_pca = np.empty((self.d, self.data.shape[1]))\n",
    "        eigvalues = w\n",
    "        for i in range(self.d):\n",
    "            W_pca[i] = v[:,ind[i]]\n",
    "        \n",
    "        return W_pca.real, eigvalues # discard imaginary part\n",
    "\n",
    "    def plot_error(self,  max_d):\n",
    "        x=range(1,max_d+1)\n",
    "        errors=[sum(self.eigvalues[d:]) for d in x]\n",
    "        plt.plot(x, errors)\n",
    "        plt.xlabel('Number of principal components $d$')\n",
    "        plt.ylabel('Reconstruction error $\\mathcal{E}$')\n",
    "        plt.title('Number of principal components vs the reconstruction error')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_scatter(self):\n",
    "        # get x for d=2\n",
    "        X_2d = np.matmul(self.W_pca[:2,:],self.data[:,:,None])[:,:,0]\n",
    "        plt.figure(1, figsize=(10, 10))   \n",
    "        plt.scatter(X_2d[:2178,0], X_2d[:2178,1], 3, marker='o', color='blue')\n",
    "        plt.scatter(X_2d[2178:,0], X_2d[2178:,1], 3, marker='x', color='red')\n",
    "        plt.xlabel('First principal component')\n",
    "        plt.ylabel('Second principal component')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def low_dim_data(self):\n",
    "        new_data = np.ndarray((self.data.shape[0], self.d))\n",
    "        for i in range(self.data.shape[0]):\n",
    "            new_data[i] = np.matmul(self.W_pca, self.data[i])\n",
    "        return new_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see roughly the ratio between the first, i.e. largest, principal components, the first five eigenvalues are printed and the reconstruction error is plotted for the first 30 principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(train_data, 50)\n",
    "print('The first 5 principal component eigenvalues:\\n', pca.eigvalues[:5].real)\n",
    "# plot the number of principal components vs the reconstruction error\n",
    "pca.plot_error(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first component is about four orders of magnitude larger than the second. A scetterplot can be created where the first two components are used as axes. To try to understand how the labels are distributed in this space, we colour the Pop-rock genre as blue and the rest red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.plot_scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not show clear division or clustering of the labels in this 2D space. The data can be scaled with a scikit-learn method and we try the PCA again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaled_data = preprocessing.scale(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_scaled = PCA(scaled_data, 50)\n",
    "print(pca_scaled.eigvalues[:5].real)\n",
    "# plot the number of principal components vs the reconstruction error\n",
    "pca_scaled.plot_error(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scaled.plot_scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this scatterplot can be seen a slight shift in the direction of the 1st and 2nd principal components if we compare the Pop-rock class to the other classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*- Explain your whole approach (you can include a block diagram showing the steps in your process).* \n",
    "\n",
    "*- What methods/algorithms, why were the methods chosen. *\n",
    "\n",
    "*- What evaluation methodology (cross CV, etc.).*\n",
    "\n",
    "First we tried some basic classifiers from the scikit-learn library:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciKit-learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is divided to training and validatation data after suffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "\n",
    "temp = np.hstack((labels, scaled_data))\n",
    "np.random.shuffle(temp)\n",
    "num = int(ratio * temp.shape[0])\n",
    "train_data = temp[:num, 1:]\n",
    "train_labels = temp[:num, :1]\n",
    "val_data = temp[num:, 1:]\n",
    "val_labels = temp[num:, :1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='saga',multi_class='multinomial', max_iter=100, verbose=1)\n",
    "\n",
    "model = clf.fit(train_data, train_labels)\n",
    "prediction = model.predict(val_data)\n",
    "\n",
    "# plot label distribution in prediction\n",
    "print(\"train score: \", clf.score(train_data, train_labels))\n",
    "print(\"test score: \", clf.score(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep network with Keras\n",
    "\n",
    "We wanted to test how a modern neural network library performs. We first separated the data into thee distrinct categories according to the data description: rhythm, chroma and MFCC. We then put each of these in separate inputs and made a shallow network to interpret the data. Afterward we combined the results into a single layer which was followed by a deep layer composition. The hyperparameters were chosen mainly by testing out different ones and looking for the best ones. Last layer uses softmax activation explicitly since we wanted to use the same results for both competitions. Log-loss values were the direct values from the last layer and labels where chosen by the largest element in the 10-long label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, LSTM, concatenate, Dense, Dropout, BatchNormalization, GRU, GaussianNoise\n",
    "from preprocessor import Preprocessor\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1_l2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "dl = Preprocessor(balance=False, scale=True, mutation_rate=0.15)\n",
    "x_train, y_train, x_test, y_test = dl.divided_data(ratio=0.8, load_bal_data=False)\n",
    "y_train = dl.transformed_labels(y_train)\n",
    "y_test = dl.transformed_labels(y_test)\n",
    "\n",
    "rhythm_input = Input(shape=(168,)) # timesteps, timestep dimension\n",
    "chroma_input = Input(shape=(48,)) # timesteps, timestep dimension\n",
    "mfcc_input \t = Input(shape=(48,)) # timesteps, timestep dimension\n",
    "\n",
    "rhythm  = Dense(168, activation='tanh', kernel_regularizer=l1_l2(1e-6, 1e-6))(rhythm_input)\n",
    "rhythm  = Dropout(rate=0.3)(rhythm)\n",
    "rhythm  = Dense(42, activation='softmax', kernel_regularizer=l1_l2(1e-6, 2e-6))(rhythm)\n",
    "rhythm  = Dropout(rate=0.4)(rhythm)\n",
    "\n",
    "chroma  = Dense(48, activation='tanh', kernel_regularizer=l1_l2(1e-6, 1e-6))(chroma_input)\n",
    "chroma  = Dropout(rate=0.3)(chroma)\n",
    "chroma  = Dense(48, activation='tanh', kernel_regularizer=l1_l2(1e-6, 1e-6))(chroma)\n",
    "chroma  = Dropout(rate=0.4)(chroma)\n",
    "\n",
    "mfcc  \t = Dense(48, activation='tanh', kernel_regularizer=l1_l2(1e-6, 1e-6))(mfcc_input)\n",
    "mfcc    = Dropout(rate=0.3)(mfcc)\n",
    "mfcc  \t = Dense(48, activation='tanh', kernel_regularizer=l1_l2(1e-6, 1e-6))(mfcc)\n",
    "mfcc    = Dropout(rate=0.4)(mfcc)\n",
    "\n",
    "classifier \t = concatenate([rhythm, chroma, mfcc], axis=-1)\n",
    "\n",
    "for rate in range(3):\n",
    "\tclassifier = Dense(120, activation='tanh', kernel_regularizer=l1_l2(1e-6, 1e-6))(classifier)\n",
    "\tclassifier = Dropout(rate=0.3)(classifier)\n",
    "\n",
    "classifier = Dense(10, activation='softmax', kernel_regularizer=l1_l2(1e-7, 1e-6))(classifier)\n",
    "\n",
    "input_data = [\n",
    "\tx_train[:, :168],\n",
    "\tx_train[:, 168:216],\n",
    "\tx_train[:, 216:]\n",
    "]\n",
    "\n",
    "validation_data = [\n",
    "\tx_test[:, :168],\n",
    "\tx_test[:, 168:216],\n",
    "\tx_test[:, 216:]\n",
    "]\n",
    "\n",
    "tensorboardCB = keras.callbacks.TensorBoard(log_dir='./Graph', \n",
    "                                          histogram_freq=1,  \n",
    "                                          write_graph=True, \n",
    "                                          write_images=True)\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "sgd = SGD(momentum=0.1, nesterov=True)\n",
    "\n",
    "model = Model(inputs=[rhythm_input, chroma_input, mfcc_input], outputs=classifier)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(input_data, y_train, \n",
    "\t\t\t\tvalidation_data=(validation_data, y_test), \n",
    "\t\t\t\tbatch_size=256, \n",
    "\t\t\t\tepochs=300)\n",
    "\n",
    "data = dl.test_data\n",
    "test_data = [\n",
    "\tdata[:, :168],\n",
    "\tdata[:, 168:216],\n",
    "\tdata[:, 216:]\n",
    "]\n",
    "prediction = model.predict(test_data)\n",
    "\n",
    "\n",
    "pd.DataFrame(prediction).to_csv('logloss-result.csv')\n",
    "\n",
    "y_classes = prediction.argmax(axis=-1) # convert probabilities into labels\n",
    "y_classes = np.subtract(y_classes, [-1]*6544)\n",
    "\n",
    "plt.figure(2, figsize=(10, 8))\n",
    "plt.title(\"prediction label distribution\")\n",
    "plt.hist(y_classes, range=(0.5,10.5), bins=10, ec='black')\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(y_classes).to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summarize the results of the experiments without discussing their implications.*\n",
    "\n",
    "*- Include both performance measures (accuracy and LogLoss).*\n",
    "\n",
    "*- How does it perform on kaggle compared to the train data.*\n",
    "\n",
    "*- Include a confusion matrix.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix we can see which labels are confused as each other.\n",
    "\n",
    "The code is from:\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_m = confusion_matrix(val_labels, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(__doc__)\n",
    "\n",
    "import itertools\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_confusion_matrix(cm, classes=['1','2','3','4','5','6','7','8','9','10'],\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "# cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_confusion_matrix(conf_m,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_confusion_matrix(conf_m, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion/Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpret and explain your results *\n",
    "\n",
    "*- Discuss the relevance of the performance measures (accuracy and LogLoss) for\n",
    "imbalanced multiclass datasets. *\n",
    "\n",
    "*- How the results relate to the literature. *\n",
    "\n",
    "*- Suggestions for future research/improvement. *\n",
    "\n",
    "*- Did the study answer your questions? *\n",
    "\n",
    "The neural network yielded similar results to the logistic regression with slightly higher performance. Since we wanted to use the same network for both competitions we were not able to tailor the model for the individual competitions which may affect the overall performance of the model. First problem we ran into when designing the network were that the network learned the class distribution of the training dataset instead of generalizing based on the features. Unbalanced data remains a non-trivial problem to be solved.\n",
    "\n",
    "It might be feasible to build a pipeline for the classification: since the classes are quite inbalanced (for example, class 1 being almost half of the samples) we could build a classifier that tells us whether a sample belongs to class 1 or not. Latter result makes the model advance in its pipeline to a classifier that classifies a somewhat evenly distributed dataset next (for example, classes 4-8 in the next classifier). Then it could advance to a stage where it checks whether a sample belongs to class 2 or to set of classes (3, 9, 10). Latter result would then differentiate between class 3 and set (9, 10) and then at the last stage it would identify whether the sample belongs to class 9 or class 10.\n",
    "\n",
    "Class inbalance could also be handled with creating dummy data. We tried duplicating the low-count samples and sometimes even mutating them to make them distinct from the original data but this kind of preprocessing did not give us better results. A generative adversarial network may yield sufficient results for this purpose but we did not have the time to test GANs [2].\n",
    "\n",
    "The neural net performed quite well (64% accuracy) as did the logistic regression (62% accuracy). We were unable to achieve results over 70% with the validation data (checking was done during the training). We tried different optimizers and went with Adam since it can be expected to converge faster than other optimizers [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://arxiv.org/abs/1412.6980v8\n",
    "\n",
    "[2] https://arxiv.org/abs/1406.2661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "*Any additional material needed to complete the report can be included here. For example, if you want to keep  additional source code, additional images or plots, mathematical derivations, etc. The content should be relevant to the report and should help explain or visualize something mentioned earlier. **You can remove the whole Appendix section if there is no need for it.** *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
